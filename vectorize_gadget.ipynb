{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "operators3 = {'<<=', '>>='}\n",
    "operators2 = {\n",
    "    '->', '++', '--', \n",
    "    '!~', '<<', '>>', '<=', '>=', \n",
    "    '==', '!=', '&&', '||', '+=', \n",
    "    '-=', '*=', '/=', '%=', '&=', '^=', '|='\n",
    "    }\n",
    "operators1 = { \n",
    "    '(', ')', '[', ']', '.', \n",
    "    '+', '-', '*', '&', '/', \n",
    "    '%', '<', '>', '^', '|', \n",
    "    '=', ',', '?', ':' , ';',\n",
    "    '{', '}'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GadgetVectorizer:\n",
    "\n",
    "    def __init__(self, vector_length):\n",
    "        self.gadgets = []\n",
    "        self.vector_length = vector_length\n",
    "        self.forward_slices = 0\n",
    "        self.backward_slices = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Takes a line of C++ code (string) as input\n",
    "    Tokenizes C++ code (breaks down into identifier, variables, keywords, operators)\n",
    "    Returns a list of tokens, preserving order in which they appear\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def tokenize(line):\n",
    "        tmp, w = [], []\n",
    "        i = 0\n",
    "        while i < len(line):\n",
    "            # Ignore spaces and combine previously collected chars to form words\n",
    "            if line[i] == ' ':\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i])\n",
    "                w = []\n",
    "                i += 1\n",
    "            # Check operators and append to final list\n",
    "            elif line[i:i+3] in operators3:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i:i+3])\n",
    "                w = []\n",
    "                i += 3\n",
    "            elif line[i:i+2] in operators2:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i:i+2])\n",
    "                w = []\n",
    "                i += 2\n",
    "            elif line[i] in operators1:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i])\n",
    "                w = []\n",
    "                i += 1\n",
    "            # Character appended to word list\n",
    "            else:\n",
    "                w.append(line[i])\n",
    "                i += 1\n",
    "        # Filter out irrelevant strings\n",
    "        res = list(filter(lambda c: c != '', tmp))\n",
    "        return list(filter(lambda c: c != ' ', res))\n",
    "\n",
    "    \"\"\"\n",
    "    Tokenize entire gadget\n",
    "    Tokenize each line and concatenate to one long list\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def tokenize_gadget(gadget):\n",
    "        tokenized = []\n",
    "        function_regex = re.compile('FUN(\\d)+')\n",
    "        backwards_slice = False\n",
    "        for line in gadget:\n",
    "            tokens = GadgetVectorizer.tokenize(line)\n",
    "            tokenized += tokens\n",
    "            if len(list(filter(function_regex.match, tokens))) > 0:\n",
    "                backwards_slice = True\n",
    "            else:\n",
    "                backwards_slice = False\n",
    "        return tokenized, backwards_slice\n",
    "\n",
    "    \"\"\"\n",
    "    Add input gadget to model\n",
    "    Tokenize gadget and buffer it to list\n",
    "    \"\"\"\n",
    "    def add_gadget(self, gadget):\n",
    "        tokenized_gadget, backwards_slice = GadgetVectorizer.tokenize_gadget(gadget)\n",
    "        self.gadgets.append(tokenized_gadget)\n",
    "        if backwards_slice:\n",
    "            self.backward_slices += 1\n",
    "        else:\n",
    "            self.forward_slices += 1\n",
    "\n",
    "    \"\"\"\n",
    "    Uses Word2Vec to create a vector for each gadget\n",
    "    Gets a vector for the gadget by combining token embeddings\n",
    "    Number of tokens used is min of number_of_tokens and 50\n",
    "    \"\"\"\n",
    "    def vectorize(self, gadget):\n",
    "        tokenized_gadget, backwards_slice = GadgetVectorizer.tokenize_gadget(gadget)\n",
    "        vectors = numpy.zeros(shape=(50, self.vector_length))\n",
    "        if backwards_slice:\n",
    "            for i in range(min(len(tokenized_gadget), 50)):\n",
    "                vectors[50 - 1 - i] = self.embeddings[tokenized_gadget[len(tokenized_gadget) - 1 - i]]\n",
    "        else:\n",
    "            for i in range(min(len(tokenized_gadget), 50)):\n",
    "                vectors[i] = self.embeddings[tokenized_gadget[i]]\n",
    "        return vectors\n",
    "\n",
    "    \"\"\"\n",
    "    Done adding gadgets, now train Word2Vec model\n",
    "    Only keep list of embeddings, delete model and list of gadgets\n",
    "    \"\"\"\n",
    "    def train_model(self):\n",
    "        # Set min_count to 1 to prevent out-of-vocabulary errors\n",
    "        model = Word2Vec(self.gadgets, min_count=1, size=self.vector_length, sg=1)\n",
    "        self.embeddings = model.wv\n",
    "        del model\n",
    "        del self.gadgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
